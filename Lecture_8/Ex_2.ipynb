{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - One-vs-all MNIST\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      "/Users/atakancoban/Desktop/School/2dv506 - Machine learning/Assignment 3/Lecture_8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "import pickle\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "print(\"Current working directory:\", os.getcwd(), sep=\"\\n\")\n",
    "\n",
    "# So that changes to the a3 model are reflected here.\n",
    "import a3\n",
    "import importlib\n",
    "importlib.reload(a3)\n",
    "import a3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(clfs, test):\n",
    "    preds = []\n",
    "    for i in range(len(clfs)):\n",
    "        print(clfs[i].predict(test))\n",
    "        if clfs[i].predict(test) == 1:\n",
    "            preds.append(i)\n",
    "\n",
    "def save_pickle(file_path, data):\n",
    "    file = open(file_path, 'ab')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    file = open(file_path, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1 - Load & trim MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = a3.mnist()\n",
    "\n",
    "# Trim data.\n",
    "trim = True\n",
    "if trim:\n",
    "    train_size = 10000\n",
    "    test_size = 10000\n",
    "    X_train, y_train = X_train[:train_size, :], y_train[:train_size, :]\n",
    "    X_test, y_test = X_test[:test_size, :], y_test[:test_size, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2 - Train or load SVMs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading makes results fail on my laptop atm.\n",
    "# True to load from disk, False to train models.\n",
    "load_clfs = True\n",
    "\n",
    "C = 10\n",
    "gamma = 0.01\n",
    "\n",
    "if load_clfs:\n",
    "    clfs = load_pickle('pickles/clfs.pickle')\n",
    "else:\n",
    "    # Train model for each class\n",
    "    clfs = []\n",
    "    for col in range(y_train.shape[1]):\n",
    "        print(\"Training model for class\", col)\n",
    "        clf = SVC(C=C, gamma=gamma).fit(X_train, y_train[:, col])\n",
    "        clfs.append(clf)\n",
    "\n",
    "    print()\n",
    "    save_pickle('pickles/clfs.pickle', clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 3 - Fit classifiers into OneVsAll Classifier\n",
    "### Note\n",
    "If the one-vs-all classifier predicts multiple digits but is still correct for one of them, it is considered as an\n",
    "accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating test errors for OneVsAll...\n",
      "709 errors out of 10000 predictions\n",
      "Accuracy = 92.9%\n"
     ]
    }
   ],
   "source": [
    "# Train/load OneVsAll classifier and predict training samples.\n",
    "ova_clf = a3.OneVsAllClassifier(clfs)\n",
    "\n",
    "print(\"Calculating test errors for OneVsAll...\")\n",
    "y_pred_ova = ova_clf.predict(X_test)\n",
    "\n",
    "# Calculate error count.\n",
    "y_diff_ova = y_test - y_pred_ova\n",
    "pred_count = int(y_test.shape[0])\n",
    "error_ova = np.where(y_diff_ova == 1)[0].shape[0]  # False negatives are 1.\n",
    "print(f\"{error_ova} errors out of {pred_count} predictions\")\n",
    "\n",
    "# Calculate accuracy.\n",
    "accuracy_ova = round((pred_count - error_ova) / pred_count, 3)\n",
    "print(f\"Accuracy = {accuracy_ova*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 4 - Train sklearn one-vs-one SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training OneVsOne classifier\n",
      "Done...\n",
      "\n",
      "Calculating test errors for OneVsOne...\n",
      "349 errors out of 10000 predictions\n",
      "Accuracy = 96.5%\n"
     ]
    }
   ],
   "source": [
    "# Reformat y from 2d to 1d to work with sklearn one-vs-one classifier.\n",
    "y_train_ovo = np.zeros(y_train.shape[0])\n",
    "for i in range(y_train.shape[0]):\n",
    "    y_train_ovo[i] = np.where(y_train[i] == 1)[0][0]\n",
    "\n",
    "print(\"Training OneVsOne classifier\")\n",
    "# Train/load OneVsOne classifier and predict training samples.\n",
    "ovo_clf = OneVsOneClassifier(SVC(C=C, gamma=gamma)).fit(X_train, y_train_ovo)\n",
    "print(\"Done...\\n\")\n",
    "\n",
    "y_pred_ovo = ovo_clf.predict(X_test)\n",
    "\n",
    "print(\"Calculating test errors for OneVsOne...\")\n",
    "\n",
    "# Reformat y from 2d to 1d to match sklearn one-vs-one classifier standard.\n",
    "y_test_ovo = np.zeros(y_test.shape[0])\n",
    "for i in range(y_test.shape[0]):\n",
    "    y_test_ovo[i] = np.where(y_test[i] == 1)[0][0]\n",
    "\n",
    "# Error count.\n",
    "y_diff_ovo = (y_test_ovo != y_pred_ovo).astype(int)\n",
    "error_ovo = np.sum(y_diff_ovo)\n",
    "print(f\"{error_ovo} errors out of {pred_count} predictions\")\n",
    "\n",
    "# Calculate accuracy.\n",
    "accuracy_ovo = round((pred_count - error_ovo) / pred_count, 3)\n",
    "print(f\"Accuracy = {accuracy_ovo*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 5 - Evaluation of multi-class classification strategies\n",
    "### Confusion matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: real response.\n",
      "Rows: predicted response.\n",
      "\n",
      "OneVsAll confusion matrix\n",
      "      0     1    2    3    4    5    6    7    8    9\n",
      "--  ---  ----  ---  ---  ---  ---  ---  ---  ---  ---\n",
      " 0  955     0    2    0    0    5    3    0    0    1\n",
      " 1    0  1113    2    0    0    0    3    5    2    3\n",
      " 2    1     2  951    2    2    0    0   10    3    0\n",
      " 3    0     2    2  926    0    3    0    0   10    1\n",
      " 4    1     0    5    1  928    2    7    4    2   18\n",
      " 5    3     0    0   10    0  814    8    0    4    2\n",
      " 6    2     2    6    1    6    6  905    0    1    1\n",
      " 7    1     0   10    8    1    1    0  944    2    6\n",
      " 8    3     1    8    7    4    7    2    0  857    0\n",
      " 9    2     0    4    3   24    1    0   20    0  898\n",
      "\n",
      "OneVsOne confusion matrix\n",
      "      0     1    2    3    4    5    6    7    8    9\n",
      "--  ---  ----  ---  ---  ---  ---  ---  ---  ---  ---\n",
      " 0  967     0    6    0    1    6    4    0    3    4\n",
      " 1    0  1125    0    0    0    1    3    8    1    6\n",
      " 2    2     3  999   12    3    1    4   18    2    1\n",
      " 3    0     0    2  969    0   21    0    6    9    8\n",
      " 4    0     0    2    1  952    4    4    5    6   17\n",
      " 5    2     1    0    6    0  840    4    0    8    5\n",
      " 6    6     4    5    1    8    9  936    0    0    0\n",
      " 7    1     1    9    6    1    1    0  974    3    9\n",
      " 8    2     1    7    8    2    7    3    2  938    8\n",
      " 9    0     0    2    7   15    2    0   15    4  951\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns: real response.\")\n",
    "print(\"Rows: predicted response.\\n\")\n",
    "\n",
    "# OneVsAll\n",
    "matrix_ova = np.zeros((10,10))\n",
    "for digit in range(len(matrix_ova[0])):\n",
    "    y_digit = y_test[:, digit]\n",
    "    y_pred_digit = y_pred_ova[np.where(y_digit == 1)[0]]\n",
    "    matrix_col = np.sum(y_pred_digit, axis=0)\n",
    "    matrix_ova[:, digit] = matrix_col\n",
    "\n",
    "print(\"OneVsAll confusion matrix\")\n",
    "print(tabulate(matrix_ova, headers=np.arange(10), showindex=\"always\"))\n",
    "\n",
    "# OneVsOne\n",
    "matrix_ovo = np.zeros((10,10))\n",
    "for digit in range(len(matrix_ovo[0])):\n",
    "    y_pred_digit = y_pred_ovo[np.where(y_test_ovo == digit)[0]]\n",
    "    digits, counts = np.unique(y_pred_digit, return_counts=True)\n",
    "    matrix_col = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        if i in digits:\n",
    "            matrix_col[i] = counts[np.where(digits == i)[0]]\n",
    "\n",
    "    matrix_ovo[:, digit] = matrix_col\n",
    "\n",
    "print(\"\\nOneVsOne confusion matrix\")\n",
    "print(tabulate(matrix_ovo, headers=np.arange(10), showindex=\"always\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "Overall, both classifiers perform quite well. The test results show that:\n",
    "1. OneVsAll has an accuracy of 92.9% whereas OneVsOne has an accuracy of 96.5%\n",
    "2. OneVsOne had nearly half the number of prediction errors for the tests ->\n",
    "    OneVsOne had 349 errors and OneVsAll had 709.\n",
    "\n",
    "The confusion matrices appear to be quite similar for both classifiers with similar patterns of\n",
    "errors. Digits that in nature appear similar get predicted incorrectly more often than those\n",
    "that do not. Some examples are:\n",
    "* 4 predicted as 9.\n",
    "* 7 predicted as 9.\n",
    "* 8 predicted as 3.\n",
    "\n",
    "Variance does exist between the two classifiers, however. OneVsOne classifies 5s as 3s and 3s as 2s\n",
    "significantly more often than OneVsAll.\n",
    "\n",
    "Picking a classifier, therefore, becomes a situational matter. If predicting 5s as 3s is\n",
    "a more costful error to the user than having less overall accuracy, the user may\n",
    "choose to use the OneVsAll classifier. Likewise, if overall accuracy matters most to another user,\n",
    "they may choose to use the OneVsOne classifier.\n",
    "\n",
    "Finally, it is important to note that the OneVsOne classifier takes significantly longer to predict\n",
    "than OneVsAll since it consists of more classifiers.\n",
    "* The OneVsAll classifier is bundled with 10 classifiers.\n",
    "* The OneVsOne classifier is bundled with 45 classifiers.\n",
    "\n",
    "This means that the OneVsOne classifier has to predict 4.5x more than the OneVsAll classifier which is a significant\n",
    "increase in required computational power."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}